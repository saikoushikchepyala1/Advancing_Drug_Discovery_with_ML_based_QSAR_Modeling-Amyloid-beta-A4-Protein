# -*- coding: utf-8 -*-
"""ml_based QSAR_modeling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Opr8sHnWMYaP1zE2199iUZkLC6NlFStF
"""

! pip install chembl_webresource_client

import pandas as pd
from chembl_webresource_client.new_client import new_client

target = new_client.target
target_query = target.search('Alzheimers')
targets = pd.DataFrame.from_dict(target_query)
targets

selected_target = targets.target_chembl_id[5]
selected_target

activity = new_client.activity
res = activity.filter(target_chembl_id=selected_target).filter(standard_type="IC50")
df = pd.DataFrame.from_dict(res)
df

df.to_csv('bioactivity_data_raw.csv', index=False)
df2 = df[df.standard_value.notna()]
df2

bioactivity_class = []
for i in df2.standard_value:
  if float(i) >= 10000:
    bioactivity_class.append("inactive")
  elif float(i) <= 1000:
    bioactivity_class.append("active")
  else:
    bioactivity_class.append("intermediate")

selection = ['molecule_chembl_id','canonical_smiles','standard_value']
df3 = df2[selection]
df3

bioactivity_class = pd.Series(bioactivity_class, name='bioactivity_class')
df4 = pd.concat([df3, bioactivity_class], axis=1)
df4.dropna(inplace=True)
df4

df4.to_csv('bioactivity_data_preprocessed.csv', index=False)
df4

! pip install rdkit

import pandas as pd
import numpy as np
from rdkit import Chem
from rdkit.Chem import Descriptors, Lipinski

df = pd.read_csv('/content/bioactivity_data_preprocessed.csv')

df = df[df['canonical_smiles'].notna()]
df

def lipinski(smiles, verbose=False):
    moldata = []
    for elem in smiles:
        mol = Chem.MolFromSmiles(elem)
        moldata.append(mol)


    baseData = np.arange(1,1)
    i = 0
    for mol in moldata:
        desc_MolWt = Descriptors.MolWt(mol)
        desc_MolLogP = Descriptors.MolLogP(mol)
        desc_NumHDonors = Lipinski.NumHDonors(mol)
        desc_NumHAcceptors = Lipinski.NumHAcceptors(mol)

        row = np.array([desc_MolWt,
                        desc_MolLogP,
                        desc_NumHDonors,
                        desc_NumHAcceptors])

        if(i == 0):
            baseData = row
        else:
            baseData = np.vstack([baseData, row])
        i = i + 1

    columnNames = ["MW", "LogP", "NumHDonors", "NumHAcceptors"]
    descriptors = pd.DataFrame(data=baseData, columns=columnNames)

    return descriptors

df_lipinski = lipinski(df.canonical_smiles)
df_lipinski

df_combined = pd.concat([df, df_lipinski], axis=1)
df_combined

def pIC50(input):
    pIC50 = []
    for i in input['standard_value_norm']:
        molar = i * (10**-9)
        if molar > 0:
            pIC50.append(-np.log10(molar))
        else:
            pIC50.append(np.nan)

    input['pIC50'] = pIC50
    x = input.drop('standard_value_norm', axis=1)
    return x

df_combined.standard_value.describe()

def norm_value(input):
    norm = []

    for i in input['standard_value']:
        if i > 100000000:
            i = 100000000
        norm.append(i)
    input['standard_value_norm'] = norm
    x = input.drop('standard_value',axis=1)

    return x

df_norm = norm_value(df_combined)
df_norm

df_norm.standard_value_norm.describe()

df_final = pIC50(df_norm)
df_final

df_final.head()

df_final.pIC50.describe()

df2_class = df_final[df_final.bioactivity_class != 'intermediate']
df2_class

df2_class.to_csv('bioactivity_data_preprocessed_final.csv')

import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import mannwhitneyu

def mannwhitney(descriptor):

    from numpy.random import seed
    from numpy.random import randn
    from scipy.stats import mannwhitneyu

    seed(1)

    selection = [descriptor, 'bioactivity_class']
    df=df2_class[selection]
    active = df[df.bioactivity_class == 'active']
    active = active[descriptor]

    selection = [descriptor, 'bioactivity_class']
    df=df2_class[selection]
    inactive = df[df.bioactivity_class == 'inactive']
    inactive = inactive[descriptor]


    stat, p = mannwhitneyu(active, inactive)


    alpha = 0.05
    if p > alpha:
        interpretation = 'Same distribution (fail to reject H0)'
    else:
        interpretation = 'Different distribution (reject H0)'

    results = pd.DataFrame({
        'Descriptor': [descriptor],
        'Statistic': [stat],
        'p-value': [p],
        'Interpretation': [interpretation]
    })


    filename = f'mannwhitneyu_{descriptor}.csv'
    results.to_csv(filename, index=False)

    return results

mannwhitney('pIC50')

mannwhitney('MW')

mannwhitney('LogP')

mannwhitney('NumHDonors')

mannwhitney('NumHAcceptors')

sns.set(style='ticks')
plt.figure(figsize=(5.5, 5.5))
sns.countplot(x='bioactivity_class', data=df2_class, edgecolor='black')
plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('Frequency', fontsize=14, fontweight='bold')
plt.savefig('plot_bioactivity_class.pdf')
plt.show()

plt.figure(figsize=(5.5, 5.5))
sns.scatterplot(x='MW', y='LogP', data=df2_class, hue='bioactivity_class', size='pIC50', edgecolor='black', alpha=0.7)
plt.xlabel('MW', fontsize=14, fontweight='bold')
plt.ylabel('LogP', fontsize=14, fontweight='bold')
plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0)
plt.savefig('plot_MW_vs_LogP.pdf')
plt.show()

plt.figure(figsize=(5.5, 5.5))
sns.boxplot(x='bioactivity_class', y='pIC50', data=df2_class)
plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('pIC50 value', fontsize=14, fontweight='bold')
plt.savefig('plot_ic50.pdf')
plt.show()

plt.figure(figsize=(5.5, 5.5))
sns.boxplot(x='bioactivity_class', y='MW', data=df2_class)
plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('MW', fontsize=14, fontweight='bold')
plt.savefig('plot_MW.pdf')
plt.show()

plt.figure(figsize=(5.5, 5.5))
sns.boxplot(x='bioactivity_class', y='LogP', data=df2_class)
plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('LogP', fontsize=14, fontweight='bold')
plt.savefig('plot_LogP.pdf')
plt.show()

plt.figure(figsize=(5.5, 5.5))
sns.boxplot(x='bioactivity_class', y='NumHDonors', data=df2_class)
plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('NumHDonors', fontsize=14, fontweight='bold')
plt.savefig('plot_NumHDonors.pdf')
plt.show()

plt.figure(figsize=(5.5, 5.5))
sns.boxplot(x='bioactivity_class', y='NumHAcceptors', data=df2_class)
plt.xlabel('Bioactivity class', fontsize=14, fontweight='bold')
plt.ylabel('NumHAcceptors', fontsize=14, fontweight='bold')
plt.savefig('plot_NumHAcceptors.pdf')
plt.show()

import pandas as pd

df3 = pd.read_csv('/content/bioactivity_data_preprocessed_final.csv')
df3.head()

!pip install padelpy

!unzip -o padel.zip

selection = ['canonical_smiles', 'molecule_chembl_id']
df3_selection = df3[selection]

df3_selection.to_csv('alzheimers_molecule.smi', sep=' ', index=False, header=False)

! cat alzheimers_molecule.smi

! cat padel.sh

! bash padel.sh

df3_X = pd.read_csv('descriptors_output.csv')
df3_X

df3_X = df3_X.drop(columns=['Name'])
df3_X

df3_Y = df3['pIC50']
df3_Y

dataset3 = pd.concat([df3_X,df3_Y], axis=1)
dataset3

dataset3.to_csv('alzheimers_bioactivity_data_pIC50_pubchem_fp_AB.csv', index=False)

import pandas as pd
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

df4 = pd.read_csv('/content/alzheimers_bioactivity_data_pIC50_Pubchem_fp_AB.csv')
df4.head()

X = df4.drop('pIC50', axis=1)
X

Y = df4["pIC50"]
Y

Y = Y.fillna(Y.mean())

from sklearn.feature_selection import VarianceThreshold

def remove_low_variance(input_data, threshold=0.1):
    selection = VarianceThreshold(threshold)
    selection.fit(input_data)
    return input_data[input_data.columns[selection.get_support(indices=True)]]

X = remove_low_variance(X, threshold=0.1)
X

X_df = pd.DataFrame(X)
corr_matrix = X_df.corr().abs()
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]
X1= X_df.drop(columns=to_drop)
X1

from sklearn.feature_selection import SelectKBest, f_regression
X_df = pd.DataFrame(X1)
k = 115
selector = SelectKBest(score_func=f_regression, k=k)
X2= selector.fit_transform(X_df, Y)
X2

selected_columns = X1.columns[selector.get_support()]

X2_df = pd.DataFrame(X2, columns=selected_columns)
X2_df.to_csv('descriptor_list.csv', index=False)
X2_df

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X2, Y, test_size=0.2, random_state=42)

X_train.shape, Y_train.shape

X_test.shape, Y_test.shape

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler

svr_model = make_pipeline(StandardScaler(), SVR())
svr_model.fit(X_train, Y_train)
Y_pred_svr = svr_model.predict(X_test)

print("\nðŸ”¹ Support Vector Regressor (SVR)")
print("RÂ² score :", r2_score(Y_test, Y_pred_svr))
print("MAE:", mean_absolute_error(Y_test, Y_pred_svr))
print("RMSE:", np.sqrt(mean_squared_error(Y_test, Y_pred_svr)))
print("MAPE:", np.mean(np.abs((Y_test - Y_pred_svr) / Y_test)) * 100)

from sklearn.ensemble import GradientBoostingRegressor

gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
gb_model.fit(X_train, Y_train)
Y_pred_gb = gb_model.predict(X_test)

print("\nðŸ”¹ Gradient Boosting Regressor")
print("RÂ² score :", r2_score(Y_test, Y_pred_gb))
print("MAE:", mean_absolute_error(Y_test, Y_pred_gb))
print("RMSE:", np.sqrt(mean_squared_error(Y_test, Y_pred_gb)))
print("MAPE:", np.mean(np.abs((Y_test - Y_pred_gb) / Y_test)) * 100)

from xgboost import XGBRegressor

xgb_model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)
xgb_model.fit(X_train, Y_train)
Y_pred_xgb = xgb_model.predict(X_test)

print("\nðŸ”¹ XGBoost Regressor")
print("RÂ² score :", r2_score(Y_test, Y_pred_xgb))
print("MAE:", mean_absolute_error(Y_test, Y_pred_xgb))
print("RMSE:", np.sqrt(mean_squared_error(Y_test, Y_pred_xgb)))
print("MAPE:", np.mean(np.abs((Y_test - Y_pred_xgb) / Y_test)) * 100)

from sklearn.ensemble import VotingRegressor

voting_model = VotingRegressor(estimators=[
    ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
    ('xgb', XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42))
])
voting_model.fit(X_train, Y_train)

Y_pred_vote = voting_model.predict(X_test)

print("\nðŸ”¹ Voting Regressor")
print("RÂ² score :", r2_score(Y_test, Y_pred_vote))
print("MAE:", mean_absolute_error(Y_test, Y_pred_vote))
print("RMSE:", np.sqrt(mean_squared_error(Y_test, Y_pred_vote)))
print("MAPE:", np.mean(np.abs((Y_test - Y_pred_vote) / Y_test)) * 100)

from sklearn.ensemble import BaggingRegressor

bag_model = BaggingRegressor(n_estimators=100, random_state=42)
bag_model.fit(X_train, Y_train)
Y_pred_bag = bag_model.predict(X_test)

print("\nðŸ”¹ Bagging Regressor")
print("RÂ² score :", r2_score(Y_test, Y_pred_bag))
print("MAE:", mean_absolute_error(Y_test, Y_pred_bag))
print("RMSE:", np.sqrt(mean_squared_error(Y_test, Y_pred_bag)))
print("MAPE:", np.mean(np.abs((Y_test - Y_pred_bag) / Y_test)) * 100)

from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
rf_model.fit(X_train, Y_train)
Y_pred_rf = rf_model.predict(X_test)

print("\nðŸ”¹ Random Forest Regressor")
print("RÂ² score :", r2_score(Y_test, Y_pred_rf))
print("MAE:", mean_absolute_error(Y_test, Y_pred_rf))
print("RMSE:", np.sqrt(mean_squared_error(Y_test, Y_pred_rf)))
print("MAPE:", np.mean(np.abs((Y_test - Y_pred_rf) / Y_test)) * 100)

from sklearn.svm import SVR
from sklearn.ensemble import GradientBoostingRegressor, VotingRegressor, BaggingRegressor, RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error, mean_absolute_percentage_error
import pandas as pd
import numpy as np
from tabulate import tabulate

models = {
    "Gradient Boosting": GradientBoostingRegressor(n_estimators=100, random_state=42),
    "SVR": SVR(),
    "XGBoost": XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42),
    "Bagging Regressor": BaggingRegressor(n_estimators=100, random_state=42),
    "Voting Regressor": VotingRegressor(estimators=[
        ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),
        ('xgb', XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)),
    ]),
    "Random Forest": RandomForestRegressor(n_estimators=100, random_state=42)
}

results = []

for name, model in models.items():
    model.fit(X_train, Y_train)
    y_pred = model.predict(X_test)

    r2 = r2_score(Y_test, y_pred)
    mae = mean_absolute_error(Y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(Y_test, y_pred))
    mape = mean_absolute_percentage_error(Y_test, y_pred) * 100  # %

    results.append({
        "Model": name,
        "RÂ² Score": round(r2, 4),
        "MAE": round(mae, 3),
        "RMSE": round(rmse, 3),
        "MAPE": f"{round(mape, 2)} %"
    })


results_df = pd.DataFrame(results)
print("\n Model Performance Comparison:\n")
print(tabulate(results_df, headers='keys', tablefmt='fancy_grid', showindex=False))

Y_pred = rf_model.predict(X_test)
Y_pred

import matplotlib.pyplot as plt

Y_pred_rf = rf_model.predict(X_test)

r2 = r2_score(Y_test, Y_pred_rf)

plt.figure(figsize=(5, 5))
plt.scatter(Y_test, Y_pred_rf, alpha=0.5, color='blue', label='Predictions')
plt.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=2, label='Ideal Fit')
plt.xlabel("Actual pIC50")
plt.ylabel("Predicted pIC50")

plt.title(f"Actual vs Predicted pIC50 (Random Forest)\nRÂ² = {r2:.2f}")
plt.legend()
plt.grid(True)
plt.savefig('Actual vs Predicted pIC50 (Random Forest).pdf')
plt.show()

import shap
import matplotlib.pyplot as plt

explainer_rf = shap.Explainer(rf_model, X_train)

shap_values_rf = explainer_rf(X_test)

plt.title("SHAP Summary Plot (Beeswarm) - Random Forest", fontsize=14)
shap.plots.beeswarm(shap_values_rf, max_display=20)

import shap
import matplotlib.pyplot as plt

explainer_rf = shap.Explainer(rf_model, X_train)

shap_values_rf = explainer_rf(X_test)

plt.title("SHAP Feature Importance (Bar Plot) - Random Forest", fontsize=6)
shap.summary_plot(shap_values_rf, X_test, plot_type="bar")

plt.title("SHAP Feature Importance (Bar Plot) - Random Forest", fontsize=14)
shap.plots.bar(shap_values_rf, max_display=20)

! zip -r results.zip . -i *.csv *.pdf

import joblib
import pickle
filename = 'bioactivity_prediction_model.pkl'
pickle.dump(rf_model, open('bioactivity_prediction_model.pkl', 'wb'))